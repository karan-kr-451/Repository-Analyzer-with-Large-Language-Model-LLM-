{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_github_repo(repo_url, target_directory=None):\n",
    "    if target_directory is None:\n",
    "        target_directory = repo_url.split('/')[-1].replace('.git', '')\n",
    "    \n",
    "    if os.path.exists(target_directory):\n",
    "        print(f\"Error: Directory '{target_directory}' already exists.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(['git', 'clone', repo_url, target_directory], check=True)\n",
    "        print(f\"Successfully cloned repository to {target_directory}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: Failed to clone repository. {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = 'https://github.com/karan-kr-451/Health-Care-With-Gemini.git'\n",
    "download_github_repo(repo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created and saved to ./vector_db\n",
      "1. Install Python 3.7+ on your system.\n",
      "2. Install the required packages listed in `requirements.txt`, including Streamlit, OpenAI, and PyTorch.\n",
      "3. Clone or download the repository to your local machine.\n",
      "4. Navigate to the project directory and run `streamlit run app.py` to start the application.\n",
      "\n",
      "## Usage Instructions\n",
      "\n",
      "To use this application:\n",
      "\n",
      "1. Upload a medical report image to the Streamlit interface.\n",
      "2. Select the 'gemini-1.5-flash' model and configure its settings as needed.\n",
      "3. Click the \"Generate Analysis\" button to receive detailed analysis of the uploaded image.\n",
      "4. Visualize the results alongside the original report.\n",
      "\n",
      "## Contributing Guidelines\n",
      "\n",
      "Contributions to this project are welcome! To contribute, please follow these guidelines:\n",
      "\n",
      "* Fork the repository to your local machine.\n",
      "* Make changes to the code and commit them with descriptive commit messages.\n",
      "* Submit a pull request with your changes for review.\n",
      "\n",
      "## License Information\n",
      "If the response is generated successfully, it is displayed on the application's output section for further analysis.\n",
      "\n",
      "Code Snippets\n",
      "-------------\n",
      "\n",
      "```python\n",
      "# Load environment variables and configure Generative AI API\n",
      "load_dotenv()\n",
      "genai.configure(api_key=os.getenv(\"API_KEY\"))\n",
      "\n",
      "# Instantiate Generative Model\n",
      "model = genai.GenerativeModel(model_name='gemini-1.5-flash',\n",
      "                              generation_config=generation_config,\n",
      "                              safety_settings=safety_settings)\n",
      "```\n",
      "\n",
      "```python\n",
      "# Set page configuration and title\n",
      "st.set_page_config(page_title=\"Health Care Assistant\", \n",
      "                  layout=\"wide\")\n",
      "st.title(\"Health Care Assistant\")\n",
      "\n",
      "# Allow users to upload images for analysis\n",
      "file_uploaded = st.file_uploader('Upload the image for Analysis', \n",
      "                                  type=['png','jpg','jpeg'])\n",
      "```\n",
      "\n",
      "```python\n",
      "# Display uploaded image if file is available\n",
      "if file_uploaded:\n",
      "    st.image(file_uploaded, width=200, caption='Uploaded Image')\n",
      "```\n",
      "*   `name`: The name of the package, which in this case is \"HealthCareBot-With-Gemini\".\n",
      "*   `version`: The version number of the package, set to \"0.0.1\" in this example.\n",
      "*   `author` and `author_email`: The author's name and email address, respectively.\n",
      "*   `packages`: A list of packages that are included in the distribution, which is generated using the `find_packages()` function from `setuptools`.\n",
      "*   `install_requires`: An empty list of dependencies required to install and run the package.\n",
      "\n",
      "### find_packages() function\n",
      "\n",
      "The `find_packages()` function is used to automatically discover and include all Python packages (directories containing an `__init__.py` file) in the current directory. This function returns a list of tuples, where each tuple contains the name of a package as a string.\n",
      "\n",
      "**Important Logic/Algorithms**\n",
      "------------------------------\n",
      "\n",
      "### Package Discovery\n",
      "]\n",
      "\n",
      "for filepath in list_of_files:\n",
      "    # Split the file path into its directory and filename components\n",
      "    filepath = Path(filepath)\n",
      "    filedir, filename = os.path.split(filepath)\n",
      "\n",
      "    # Create the directory if it does not exist\n",
      "    if filedir != \"\":\n",
      "        os.makedirs(filedir, exist_ok=True)  # Create directory with or without existing\n",
      "        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\n",
      "\n",
      "    # Check if the file exists and create an empty one if necessary\n",
      "    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\n",
      "        with open(filepath, 'w') as f:\n",
      "            pass  # Create a new empty file\n",
      "            logging.info(f\"Creating empty file: {filepath}\")\n",
      "    else:\n",
      "        logging.info(f\"{filename} is already exists\")\n",
      "```\n",
      "\n",
      "**Important Logic and Algorithms**\n",
      "\n",
      "This script employs the following algorithms:\n"
     ]
    }
   ],
   "source": [
    "def process_repository(repo_path, db_path):\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2:1b\")\n",
    "    \n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "    \n",
    "    documents = []\n",
    "\n",
    "    for root, _, files in os.walk(repo_path):\n",
    "        for file in files:\n",
    "            if file.endswith(('.txt', '.md', '.py', '.js', '.html', '.css', '.yml', '.yaml')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    loader = TextLoader(file_path, encoding='utf-8')\n",
    "                    documents.extend(loader.load_and_split(text_splitter))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    \n",
    "\n",
    "    vectorstore.save_local(db_path)\n",
    "    \n",
    "    print(f\"Vector database created and saved to {db_path}\")\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "# Example usage\n",
    "repo_path = \"D:\\Github Project\\Readme Writer\\Health-Care-With-Gemini\"\n",
    "db_path = \"./vector_db\"\n",
    "\n",
    "vectorstore = process_repository(repo_path, db_path)\n",
    "\n",
    "\n",
    "query = \"Example\"\n",
    "results = vectorstore.similarity_search(query)\n",
    "for doc in results:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"streamlit\"\n",
    "results = vectorstore.similarity_search(query)\n",
    "for doc in results:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_repository(repo_path):\n",
    "    ollama = Ollama(model=\"llama3.2\", temperature=0.7)\n",
    "    \n",
    "\n",
    "    file_contents = {}\n",
    "    file_summaries = {}\n",
    "\n",
    "    for root, _, files in os.walk(repo_path):\n",
    "        for file in files:\n",
    "            if file.endswith(('.py', '.js', '.html', '.css', '.md', '.txt', '.yml', '.yaml')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(file_path, repo_path)\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        file_contents[relative_path] = content\n",
    "                        \n",
    "                        # Generate summary for each file\n",
    "                        summary_prompt = PromptTemplate(\n",
    "                            input_variables=[\"content\"],\n",
    "                            template=\"Summarize the following code or text content in a few sentences:\\n\\n{content}\"\n",
    "                        )\n",
    "                        summary_chain = LLMChain(llm=ollama, prompt=summary_prompt)\n",
    "                        summary = summary_chain.run(content=content)\n",
    "                        file_summaries[relative_path] = summary.strip()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    return file_contents, file_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_readme(repo_path, file_summaries):\n",
    "    ollama = Ollama(model=\"llama3.2\", temperature=0.7)\n",
    "    \n",
    "    readme_prompt = PromptTemplate(\n",
    "        input_variables=[\"repo_name\", \"summaries\"],\n",
    "        template=\"\"\"Generate a README.md file for a repository named {repo_name}.\n",
    "            Use the provided summaries to create an organized and comprehensive overview of the project and its key components.\n",
    "\n",
    "            The README should include the following sections:\n",
    "\n",
    "            Project Title: Display the title of the project.\n",
    "            Brief Description: Provide a concise overview of the project's purpose and goals in 50 to 100 words.\n",
    "            Table of Contents: Help users quickly navigate to different sections of the README.\n",
    "            Project Structure: Explain the organization of files and directories within the repository.\n",
    "            Installation Instructions: Step-by-step instructions to set up the project locally.\n",
    "            Usage Examples: Demonstrate how to use the project, including any relevant code snippets or examples.\n",
    "            Contributing Guidelines: Outline how others can contribute to the project.\n",
    "            License Information: Specify the license under which the project is released.\n",
    "            Contact Information: Provide details for users to reach out for support or inquiries.\n",
    "            Use the following file summaries to inform the structure and details of the README:\n",
    "            {summaries}\"\"\")\n",
    "    \n",
    "    readme_chain = LLMChain(llm=ollama, prompt=readme_prompt)\n",
    "    repo_name = os.path.basename(repo_path)\n",
    "    summaries_text = json.dumps(file_summaries, indent=2)\n",
    "    readme_content = readme_chain.run(repo_name=repo_name, summaries=summaries_text)\n",
    "    \n",
    "    with open(os.path.join(repo_path, 'README.md'), 'w', encoding='utf-8') as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    print(\"README.md generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code_documentation(repo_path, file_contents):\n",
    "    ollama = Ollama(model=\"llama3.2\",temperature=0.9)\n",
    "    \n",
    "    doc_prompt = PromptTemplate(\n",
    "        input_variables=[\"file_path\", \"content\"],\n",
    "        template=\"Generate detailed documentation for the following code file. Include an overview of the file's purpose, any classes or functions defined, and important logic or algorithms:\\n\\nFile: {file_path}\\n\\nContent:\\n{content}\"\n",
    "    )\n",
    "    \n",
    "    doc_chain = LLMChain(llm=ollama, prompt=doc_prompt)\n",
    "    \n",
    "    documentation = {}\n",
    "    for file_path, content in file_contents.items():\n",
    "        if file_path.endswith(('.py', '.js')):\n",
    "            doc = doc_chain.run(file_path=file_path, content=content)\n",
    "            documentation[file_path] = doc.strip()\n",
    "    \n",
    "    doc_file_path = os.path.join(repo_path, 'CODE_DOCUMENTATION.md')\n",
    "    with open(doc_file_path, 'w', encoding='utf-8') as f:\n",
    "        for file_path, doc in documentation.items():\n",
    "            f.write(f\"# {file_path}\\n\\n{doc}\\n\\n---\\n\\n\")\n",
    "    \n",
    "    print(\"CODE_DOCUMENTATION.md generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_code(repo_path, file_contents):\n",
    "    ollama = Ollama(model=\"llama3.2\", temperature=0.7)\n",
    "    memory = ConversationBufferMemory(return_messages=True, input_key=\"human_input\")\n",
    "    \n",
    "    chat_prompt = PromptTemplate(\n",
    "        input_variables=[\"history\", \"human_input\", \"repo_content\"],\n",
    "        template=\"You are an AI assistant specialized in code analysis, improvement, and bug fixing. Use the following repository content as context:\\n\\n{repo_content}\\n\\nConversation history:\\n{history}\\nHuman: {human_input}\\nAI:\"\n",
    "    )\n",
    "    \n",
    "    chat_chain = LLMChain(llm=ollama, prompt=chat_prompt, memory=memory)\n",
    "    \n",
    "    repo_content = json.dumps(file_contents, indent=2)\n",
    "    \n",
    "    print(\"Chat with your code assistant. Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        response = chat_chain({\"human_input\": user_input, \"repo_content\": repo_content})\n",
    "        print(\"AI:\", response['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "repo_path = \"D:\\Github Project\\Readme Writer\\Health-Care-With-Gemini\"\n",
    "\n",
    "file_contents, file_summaries = analyze_repository(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md generated successfully.\n"
     ]
    }
   ],
   "source": [
    "generate_readme(repo_path, file_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE_DOCUMENTATION.md generated successfully.\n"
     ]
    }
   ],
   "source": [
    "generate_code_documentation(repo_path, file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_with_code(repo_path, file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_code(repo_path, file_contents):\n",
    "    ollama = Ollama(model=\"llama3.2\", temperature=0.7)\n",
    "    memory = ConversationBufferMemory(return_messages=True, input_key=\"human_input\")\n",
    "    \n",
    "    chat_prompt = PromptTemplate(\n",
    "        input_variables=[\"history\", \"human_input\", \"repo_content\"],\n",
    "        template=\"You are an AI assistant specialized in code analysis, improvement, and bug fixing. Use the following repository content as context:\\n\\n{repo_content}\\n\\nConversation history:\\n{history}\\nHuman: {human_input}\\nAI:\"\n",
    "    )\n",
    "    \n",
    "    chat_chain = LLMChain(llm=ollama, prompt=chat_prompt, memory=memory)\n",
    "    \n",
    "    repo_content = json.dumps(file_contents, indent=2)\n",
    "    \n",
    "    print(\"Chat with your code assistant. Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        response = chat_chain({\"human_input\": user_input, \"repo_content\": repo_content})\n",
    "        print(\"AI:\", response['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "def solveBot(file_contents):\n",
    "    # Initialize the Ollama model (assuming it works similarly to OpenAI models)\n",
    "    ollama = Ollama(model='llama3.2', temperature=0.7)\n",
    "    \n",
    "    # Set up memory for conversation (limiting to the last 5 exchanges)\n",
    "    memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "    # System message that sets the context for the AI assistant\n",
    "    system_message = f\"You are an AI assistant specialized in code analysis, improvement, and bug fixing. Use the following repository content as context:\\n\\n{file_contents}\\n\\n\"\n",
    "    \n",
    "    # Chat prompt template for the conversation\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_message),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),   # This expects user input\n",
    "        (\"ai\", \"\")  # AI response\n",
    "    ])\n",
    "\n",
    "    # Set up the chain with the language model and the prompt template\n",
    "    chain = LLMChain(llm=ollama, prompt=chat_prompt, memory=memory)\n",
    "\n",
    "    # Initialize a response cache for performance improvement\n",
    "    response_cache = {}\n",
    "\n",
    "    # Function to create a cache key based on user input and conversation history\n",
    "    def get_cache_key(user_input):\n",
    "        history = str(memory.chat_memory.messages)  # Get the conversation history\n",
    "        return hashlib.md5((user_input + history).encode()).hexdigest()\n",
    "\n",
    "    # Function to get AI response, using caching for repeated questions\n",
    "    def get_ai_response(user_input):\n",
    "        cache_key = get_cache_key(user_input)\n",
    "        \n",
    "        # Check if the response is already in cache\n",
    "        if cache_key in response_cache:\n",
    "            print(\"(Cached response)\")\n",
    "            return response_cache[cache_key]\n",
    "        \n",
    "        # Invoke the language model chain with user input\n",
    "        response = chain.invoke({\"input\": user_input})  # Ensure \"input\" matches the template\n",
    "        \n",
    "        # Cache the response for future use\n",
    "        response_cache[cache_key] = response['text'] if 'text' in response else response\n",
    "        \n",
    "        return response_cache[cache_key]\n",
    "\n",
    "    # Set up the ThreadPoolExecutor to handle asynchronous response generation\n",
    "    executor = ThreadPoolExecutor(max_workers=1)\n",
    "\n",
    "    print(\"Chat with your code assistant. Type 'exit' to end the conversation.\")\n",
    "    \n",
    "    # Main conversation loop\n",
    "    while True:\n",
    "        user_input = input(\"You: \")  # Get user input\n",
    "        if user_input.lower() == 'exit':  # Exit condition\n",
    "            break\n",
    "        \n",
    "        # Submit the AI response task to the executor for asynchronous execution\n",
    "        future = executor.submit(get_ai_response, user_input)\n",
    "        \n",
    "        # Simulate thinking while AI is processing the response\n",
    "        print(\"AI is thinking...\", end=\"\", flush=True)\n",
    "        while not future.done():\n",
    "            time.sleep(0.5)\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        \n",
    "        # Once AI is done, print the response\n",
    "        print(\"\\nAI:\", future.result())\n",
    "\n",
    "    # Shut down the executor when the conversation ends\n",
    "    executor.shutdown()\n",
    "\n",
    "# Example Usage:\n",
    "# file_contents = analyze_repository(repo_path)  # Analyze repo contents (placeholder for now)\n",
    "# solveBot(file_contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with your code assistant. Type 'exit' to end the conversation.\n",
      "AI is thinking..."
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing some input keys: {\"'app.py'\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msolveBot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 73\u001b[0m, in \u001b[0;36msolveBot\u001b[1;34m(file_contents)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# Once AI is done, print the response\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAI:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Shut down the executor when the conversation ends\u001b[39;00m\n\u001b[0;32m     76\u001b[0m executor\u001b[38;5;241m.\u001b[39mshutdown()\n",
      "File \u001b[1;32md:\\Github Project\\Readme Writer\\venv\\lib\\concurrent\\futures\\_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32md:\\Github Project\\Readme Writer\\venv\\lib\\concurrent\\futures\\_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Github Project\\Readme Writer\\venv\\lib\\concurrent\\futures\\thread.py:52\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[1;32mIn[39], line 45\u001b[0m, in \u001b[0;36msolveBot.<locals>.get_ai_response\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response_cache[cache_key]\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Invoke the language model chain with user input\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure \"input\" matches the template\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Cache the response for future use\u001b[39;00m\n\u001b[0;32m     48\u001b[0m response_cache[cache_key] \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response \u001b[38;5;28;01melse\u001b[39;00m response\n",
      "File \u001b[1;32md:\\Github Project\\Readme Writer\\venv\\lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32md:\\Github Project\\Readme Writer\\venv\\lib\\site-packages\\langchain\\chains\\base.py:158\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    153\u001b[0m     inputs,\n\u001b[0;32m    154\u001b[0m     run_id,\n\u001b[0;32m    155\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    156\u001b[0m )\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    167\u001b[0m     )\n",
      "File \u001b[1;32md:\\Github Project\\Readme Writer\\venv\\lib\\site-packages\\langchain\\chains\\base.py:290\u001b[0m, in \u001b[0;36mChain._validate_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    288\u001b[0m missing_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_keys)\u001b[38;5;241m.\u001b[39mdifference(inputs)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_keys:\n\u001b[1;32m--> 290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing some input keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Missing some input keys: {\"'app.py'\"}"
     ]
    }
   ],
   "source": [
    "solveBot(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'app.py': 'import google.generativeai as genai\\nfrom dotenv import load_dotenv\\nimport os\\nimport streamlit as st\\nfrom src.helper import *\\n\\n\\nload_dotenv()\\ngenai.configure(api_key=os.getenv(\"API_KEY\"))\\n\\nmodel = genai.GenerativeModel(model_name=\\'gemini-1.5-flash\\',\\n                              generation_config=generation_config,\\n                              safety_settings=safety_settings)\\n\\n\\nst.set_page_config(page_title=\"Health Care Assistant\", page_icon=\"ü©∫\", \\nlayout=\"wide\")\\nst.title(\"Health Care Assistant üë®\\u200d‚öïÔ∏è ü©∫ üè•\")\\nst.subheader(\"An app to help with medical analysis using images\")\\n\\nfile_uploaded = st.file_uploader(\\'Upload the image for Analysis\\', \\ntype=[\\'png\\',\\'jpg\\',\\'jpeg\\'])\\n\\nif file_uploaded:\\n    st.image(file_uploaded, width=200, caption=\\'Uploaded Image\\')\\n    \\nsubmit=st.button(\"Generate Analysis\")\\n\\nif submit:\\n\\n    image_data = file_uploaded.getvalue()\\n    \\n    image_parts = [\\n        {\\n            \"mime_type\" : \"image/jpg\",\\n            \"data\" : image_data\\n        }\\n    ]\\n    \\n#     making our prompt ready\\n    prompt_parts = [\\n        image_parts[0],\\n        system_prompts[0],\\n    ]\\n    \\n#     generate response\\n    \\n    response = model.generate_content(prompt_parts)\\n    if response:\\n        st.title(\\'Detailed analysis based on the uploaded image\\')\\n        st.write(response.text)\\n    ', 'CODE_DOCUMENTATION.md': '# app.py\\n\\n**App.py Documentation**\\n\\n**Overview**\\n\\nThis file serves as the main entry point for a web application built using Streamlit. The application aims to assist with medical analysis by analyzing images and providing detailed descriptions.\\n\\n**Imported Modules and Functions**\\n\\n* `google.generativeai` module: provides functionality for generative AI models\\n* `streamlit` module: used for building the web application\\n* `dotenv` module: loads environment variables from a `.env` file\\n* `os` module: used to access environment variables\\n* `src.helper` module (not shown): contains helper functions and constants\\n\\n**Classes Defined**\\n\\nNone\\n\\n**Functions Defined**\\n\\n* `genai.configure(api_key=os.getenv(\"API_KEY\"))`: configures the generative AI model with the provided API key\\n* `model = genai.GenerativeModel(model_name=\\'gemini-1.5-flash\\', generation_config=generation_config, safety_settings=safety_settings)`: creates a generative AI model instance\\n* `st.set_page_config(page_title=\"Health Care Assistant\", page_icon=\"ü©∫\", layout=\"wide\")`: sets the configuration for the Streamlit application page\\n* `st.title()`, `st.subheader()`, `st.image()`, and `st.button()` functions: used to render various UI elements in the application\\n\\n**Important Logic or Algorithms**\\n\\n1. **Model Configuration**: The generative AI model is configured using an API key retrieved from the environment variables.\\n2. **File Upload**: The user can upload an image file ( PNG, JPG, or JPEG) for analysis.\\n3. **Image Analysis**: When an image file is uploaded, the application generates a detailed description based on the uploaded image.\\n4. **Model Generation**: The generative AI model is used to generate a response to the uploaded image. The response contains the generated text.\\n\\n**Algorithms Used**\\n\\n1. **Generative Adversarial Networks (GANs)**: The generative AI model uses GANs to generate responses to user input (in this case, images).\\n2. **Image Analysis**: The application performs basic image analysis using standard computer vision techniques.\\n\\n**Notes and Assumptions**\\n\\n* This code assumes that the `generation_config` and `safety_settings` variables are defined in the `src.helper` module.\\n* The `system_prompts` variable is not shown in this documentation and should be defined elsewhere in the application.\\n* The generative AI model instance (`model`) is not persisted between application runs.\\n\\n---\\n\\n# setup.py\\n\\n**Setup.py Documentation**\\n==========================\\n\\n**Overview**\\n------------\\n\\nThe `setup.py` file is a crucial component of Python package development, responsible for defining the metadata and installation requirements for a project. This documentation provides an in-depth explanation of the code within this file.\\n\\n**Classes/Functions Defined**\\n-----------------------------\\n\\n*   `find_packages()`: A function from the `setuptools` library used to discover all packages in the current directory.\\n*   `setup()`: The main function that defines the metadata and installation requirements for the project.\\n\\n**Important Logic/Algorithms**\\n--------------------------------\\n\\n### Package Discovery\\n\\nThe line `packages=find_packages()` is a critical section of the code. It utilizes the `find_packages()` function to automatically discover all packages in the current directory. This ensures that the package manager can identify and install the required dependencies.\\n\\n### Installation Requirements\\n\\nThe `install_requires` parameter is an empty list, indicating that this project does not have any explicit installation requirements. If additional dependencies were needed, they would be specified here.\\n\\n**Configuration Parameters**\\n-----------------------------\\n\\n*   **name**: The name of the project (`HealthCareBot-With-Gemini`).\\n*   **version**: The version number of the project (`0.0.1`).\\n*   **author**: The name of the project author (`Karan`).\\n*   **author_email**: The email address associated with the project author (`karankum451@gmail.com`).\\n\\n**Best Practices and Recommendations**\\n------------------------------------------\\n\\n*   It is essential to keep the `install_requires` list up-to-date, especially if external dependencies are used in the project.\\n*   Make sure to test the installation process by running `pip install .` (i.e., installing from the source directory) after updating the `setup.py` file.\\n\\n**Troubleshooting and Debugging**\\n----------------------------------\\n\\nIf issues arise during package installation or setup, refer to the following common solutions:\\n\\n*   Verify that the dependencies listed in `install_requires` are correctly installed.\\n*   Check the version number specified in `version` for potential conflicts with other projects.\\n*   Ensure that the project\\'s source code and `setup.py` file are properly formatted and consistent.\\n\\n---\\n\\n# template.py\\n\\n**Template Python Script Documentation**\\n=====================================\\n\\n**Overview**\\n------------\\n\\nThis script, located in the `template.py` file, serves as a template for setting up a new project directory structure. It handles the creation of necessary directories and files, including an `.env`, `requirements.txt`, and a trial Jupyter notebook.\\n\\n**Classes and Functions**\\n------------------------\\n\\n*   None\\n\\n**Functions:**\\n\\n1.  **create_directory_structure()**: This is not explicitly defined in the code snippet but can be assumed to be the main function that sets up the project directory structure.\\n2.  **write_to_file(file_path)**: A utility function that creates a new file at the specified path and writes an empty string to it.\\n\\n**Imported Modules**\\n--------------------\\n\\n*   `os`: Provides functions for interacting with the operating system, including creating directories and checking file existence.\\n*   `pathlib`: Offers modern and efficient path manipulation capabilities.\\n*   `logging`: Enables logging of events during the script\\'s execution.\\n\\n**Important Logic/Algorithms:**\\n\\n1.  **Directory Creation:** The script checks if a directory does not already exist before attempting to create it using `os.makedirs()`. If the directory exists, an informational message is logged indicating that the directory is being reused.\\n2.  **File Existence and Size Check:** Before creating a new file, the script checks if the file already exists and has a size of zero using `os.path.getsize()`. If either condition is met, an empty string is written to the file using `with open(filepath, \\'w\\') as f: pass`.\\n\\n**Code Structure:**\\n\\nThe code starts by importing the necessary modules. It then defines a list of files that need to be created or updated in the project directory.\\n\\n```python\\nlist_of_files = [\\n    \\'src/__init__.py\\',\\n    \\'src/helper.py\\',\\n    \".env\",\\n    \"requirements.txt\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",\\n]\\n```\\n\\nThe code then iterates over each file in the list, splitting each path into a directory and filename using `os.path.split()`. It then creates the directory for the file if it does not already exist and logs an informational message indicating that the directory has been created.\\n\\n```python\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n    if filedir != \"\":\\n        # Create the directory if it doesn\\'t exist\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory: {filedir} for the file: {filename}\")\\n```\\n\\nIf the file does not exist or is empty (as determined by checking its size using `os.path.getsize()`), the script creates a new file and logs an informational message indicating that it has been created.\\n\\n```python\\nif (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n    # Create a new file if it doesn\\'t exist or is empty\\n    with open(filepath, \\'w\\') as f:\\n        pass\\n        logging.info(f\"Creating empty file: {filepath}\")\\n```\\n\\nFinally, an informational message is logged for each existing file in the project directory.\\n\\n```python\\nelse:\\n    logging.info(f\"{filename} already exists\")\\n```\\n\\n**Example Use Cases:**\\n\\n*   To create a new project with the specified directory structure, run `python template.py` (assuming Python 3.x) or `python2 template.py` (assuming Python 2.x).\\n*   After running this script, you can verify that the necessary directories and files have been created by navigating to the project\\'s root directory.\\n\\n---\\n\\n# src\\\\helper.py\\n\\n**Documentation for src/helper.py**\\n\\n**Overview**\\n\\nThe `helper.py` file is a Python script that provides configuration settings and data structures for the application. It contains two primary sections: system prompts and safety settings.\\n\\n**System Prompts**\\n\\n### Description\\n\\nThe `system_prompts` list defines the input prompts that will be used to interact with users, particularly in the context of medical analysis or clinical report interpretation. Each prompt is a multi-line string that provides clear instructions on what information to extract from the provided medical or clinical report.\\n\\n### Functionality\\n\\nThe system prompts serve as a guide for analyzing and interpreting the contents of a medical report. The user is expected to:\\n\\n1. Identify disease symptoms mentioned in the report.\\n2. Provide a detailed description of the possible disease or condition.\\n3. Offer treatment recommendations based on the identified symptoms and disease description.\\n4. Include guidance on consulting with a healthcare professional before making any medical decisions.\\n\\n### Configuration\\n\\nThe `system_prompts` list is populated with specific content that provides clarity and context for users. The prompts are designed to be concise, clear, and easy to understand, ensuring that users can effectively extract relevant information from the provided report.\\n\\n**Safety Settings**\\n\\n### Description\\n\\nThe `safety_settings` list defines the settings for detecting potentially harmful or sensitive content in user input. Each setting is a dictionary that specifies the category of harm (e.g., harassment, hate speech, sexually explicit content) and a corresponding threshold value.\\n\\n### Functionality\\n\\nThe safety settings are used to prevent users from sharing inappropriate or offensive content. The thresholds determine when to block or flag certain types of content as potentially harmful. By default, the `BLOCK_MEDIUM_AND_ABOVE` threshold is set for each category, which means that any content exceeding this level of severity will be blocked.\\n\\n### Configuration\\n\\nThe `safety_settings` list provides a flexible way to adjust the sensitivity of content detection according to specific requirements or regulatory standards. The categories and threshold values can be modified or extended as needed to accommodate changing user needs or emerging threats.\\n\\n**Configuration Options**\\n\\nThe following configuration options are defined in the `helper.py` file:\\n\\n* `generation_config`: This dictionary configures the generation settings for text output, including temperature, top-p value, top-k value, and maximum output tokens.\\n* `safety_settings`: This list of dictionaries specifies the safety settings for detecting potentially harmful content.\\n\\n**Classes or Functions**\\n\\nThe `helper.py` file does not define any classes or functions. Instead, it provides a centralized configuration structure that can be accessed throughout the application.\\n\\n**Importance Logic or Algorithms**\\n\\nThe system prompts and safety settings are essential components of the application\\'s logic, ensuring that users interact with the system in a safe and respectful manner. The safety settings algorithm uses threshold values to detect potentially harmful content, which is then used to block or flag offending user input.\\n\\n---\\n\\n# src\\\\__init__.py\\n\\n**src/__init__.py Documentation**\\n\\nOverview\\n--------\\n\\nThe `src/__init__.py` file serves as a crucial entry point for the application\\'s package. It defines key components and configuration for the entire project.\\n\\nClasses/Functions Defined\\n-------------------------\\n\\n### Package Configuration Class\\n\\n```python\\nclass AppConfig:\\n    \"\"\"\\n    Configures the application settings.\\n\\n    Attributes:\\n        debug (bool): Enables or disables debug mode.\\n        port (int): The default port number for the server.\\n        database_url (str): The URL of the database connection.\\n    \"\"\"\\n\\n    def __init__(self, debug=False, port=8080, database_url=\"\"):\\n        \"\"\"\\n        Initializes the application configuration.\\n\\n        Args:\\n            debug (bool, optional): Enables or disables debug mode. Defaults to False.\\n            port (int, optional): The default port number for the server. Defaults to 8080.\\n            database_url (str, optional): The URL of the database connection. Defaults to \"\".\\n        \"\"\"\\n        self.debug = debug\\n        self.port = port\\n        self.database_url = database_url\\n\\n    def get_config(self):\\n        \"\"\"\\n        Returns the application configuration as a dictionary.\\n\\n        Returns:\\n            dict: A dictionary containing the application settings.\\n        \"\"\"\\n        return {\\n            \"debug\": self.debug,\\n            \"port\": self.port,\\n            \"database_url\": self.database_url\\n        }\\n```\\n\\n### Database Connection Function\\n\\n```python\\ndef connect_to_database(app_config):\\n    \"\"\"\\n    Establishes a database connection using the provided configuration.\\n\\n    Args:\\n        app_config (AppConfig): The application configuration object.\\n\\n    Returns:\\n        db_connection: A dictionary representing the connected database.\\n    \"\"\"\\n\\n    # Connect to the database based on the provided URL and credentials\\n    # NOTE: This is a simplified example and actual implementation may vary depending on the database system\\n\\n    return {\\n        \"db_name\": app_config.database_url.split(\\'/\\')[-1],\\n        \"db_credentials\": {\"username\": \"\", \"password\": \"\"}\\n    }\\n```\\n\\nImportant Logic/Algorithms\\n-------------------------\\n\\n### Environment Configuration Switching\\n\\nThe `AppConfig` class allows for environment-specific configuration switching. When the application is run in a specific environment (e.g., development, production), the corresponding configuration settings can be loaded and applied.\\n\\n```python\\n# Example usage:\\nif __name__ == \"__main__\":\\n    import os\\n\\n    # Load environment-specific configuration\\n    env_config = AppConfig(\\n        debug=os.environ.get(\"DEBUG\", False),\\n        port=int(os.environ.get(\"PORT\", 8080)),\\n        database_url=os.environ.get(\"DATABASE_URL\", \"\")\\n    )\\n```\\n\\n### Database Connection Management\\n\\nThe `connect_to_database` function establishes a connection to the database using the provided configuration. This allows for seamless switching between different databases or database configurations.\\n\\n```python\\n# Example usage:\\napp_config = AppConfig()\\ndb_connection = connect_to_database(app_config)\\n```\\n\\nConclusion\\n----------\\n\\nThe `src/__init__.py` file serves as the foundation of the application\\'s package, defining key components and configuration for the entire project. The included classes and functions enable environment-specific configuration switching, database connection management, and provide a solid base for building a robust application.\\n\\n---\\n\\n', 'README.md': '# Health-Care-With-Gemini\\n==========================\\n\\n## Project Title\\n-------------\\n\\nHealth-Care-With-Gemini is a Streamlit application that utilizes Google\\'s Generative AI platform to analyze medical images and provide insights for patients.\\n\\n## Brief Description\\n-------------------\\n\\nThis project aims to create an interactive web application that allows users to upload medical reports, receive detailed analysis, and visualize the results using the pre-trained Gemini-1.5 model from the Generative AI platform. The application will provide a user-friendly interface for healthcare professionals and patients to interact with the analysis results.\\n\\n## Table of Contents\\n-------------------\\n\\n* [Project Structure](#project-structure)\\n* [Installation Instructions](#installation-instructions)\\n* [Usage Examples](#usage-examples)\\n* [Contributing Guidelines](#contributing-guidelines)\\n* [License Information](#license-information)\\n* [Contact Information](#contact-information)\\n\\n## Project Structure\\n-------------------\\n\\nThe repository is organized into the following directories:\\n\\n* `app.py`: The main application file that uses Streamlit to create the web interface.\\n* `CODE_DOCUMENTATION.md`: A detailed documentation of the code structure and implementation.\\n* `requirements.txt`: A list of dependencies required for the project.\\n* `setup.py`: A setup script for the Python package using `setuptools`.\\n* `template.py`: A script that creates a directory structure based on specified files.\\n* `src/helper.py`: A file related to text analysis tasks, specifically analyzing medical reports.\\n\\n## Installation Instructions\\n---------------------------\\n\\nTo install the project locally:\\n\\n1. Clone the repository: `git clone https://github.com/username/Health-Care-With-Gemini.git`\\n2. Install required packages: `pip install -r requirements.txt`\\n3. Run the application: `streamlit run app.py`\\n\\n## Usage Examples\\n----------------\\n\\nHere is an example of how to use the application:\\n\\n1. Open a web browser and navigate to `http://localhost:8501`.\\n2. Click on the \"Upload Medical Report\" button and select a medical report file (JPEG, PNG, or JPG).\\n3. The application will generate a detailed analysis of the medical report using the pre-trained Gemini-1.5 model.\\n4. Visualize the results by clicking on the \"View Analysis Results\" button.\\n\\n## Contributing Guidelines\\n------------------------\\n\\nContributions are welcome and encouraged. To contribute to the project:\\n\\n1. Fork the repository: `git fork https://github.com/username/Health-Care-With-Gemini.git`\\n2. Create a new branch: `git checkout -b feature/new-feature`\\n3. Make changes to the code and commit them: `git add .` and `git commit -m \"Add new feature\"`\\n4. Push changes to the forked repository: `git push origin feature/new-feature`\\n5. Open a pull request: `git request Pull Request`\\n\\n## License Information\\n--------------------\\n\\nThis project is released under the MIT license.\\n\\n## Contact Information\\n-------------------\\n\\nFor any questions or feedback, please contact:\\n\\n* Username: [username]\\n* Email: [email]\\n\\nNote: This is a generated README file based on the provided code structure and metadata. You may need to customize it according to your specific project requirements.', 'requirements.txt': 'openai\\nstreamlit\\npython-dotenv\\ngoogle-generativeai\\n-e .', 'setup.py': 'from setuptools import find_packages, setup\\n\\nsetup(\\n    name=\"HealthCareBot-With-Gemini\",\\n    version=\"0.0.1\",\\n    author=\"Karan\",\\n    author_email=\"karankum451@gmail.com\",\\n    packages=find_packages(),\\n    install_requires=[],\\n)', 'template.py': 'import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\n\\nlist_of_files = [\\n    \\'src/__init__.py\\',\\n    \\'src/helper.py\\',\\n    \".env\",\\n    \"requirements.txt\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",\\n\\n]\\n\\nfor filepath in list_of_files:\\n   filepath = Path(filepath)\\n   filedir, filename = os.path.split(filepath)\\n\\n   if filedir!=\"\":\\n      os.makedirs(filedir, exist_ok= True)\\n      logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n\\n   if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n      with open(filepath, \\'w\\') as f:\\n         pass\\n         logging.info(f\"Creating empty file: {filepath}\")\\n\\n   else:\\n      logging.info(f\"{filename} is already exists\")', 'src\\\\helper.py': 'system_prompts = [\\n    \"\"\"\\n    You are a medical expert specializing in the analysis of clinical reports. Your task is to thoroughly analyze the content of the provided medical or clinical report to extract and interpret the relevant health information. Your responsibilities include:\\n\\n    1. **Identification of Disease Symptoms**: Carefully read the report and identify any symptoms or conditions mentioned. Determine which diseases or health issues these symptoms might correspond to.\\n\\n    2. **Diagnosis and Description**: Based on the symptoms identified, provide a detailed description of the possible disease or condition. Include relevant medical terminology and contextual information from the report.\\n\\n    3. **Treatment Recommendations**: Analyze the symptoms and the disease description to suggest appropriate remedies, tests, or treatments. Indicate whether medication is necessary, and if so, specify which medications might be appropriate.\\n\\n    4. **Guidance on Medical Consultation**: Include a note that emphasizes the importance of consulting with a healthcare professional before making any medical decisions. Ensure to provide this disclaimer clearly.\\n\\n    **Important Notes to Remember:**\\n    1. **Scope of Analysis**: Focus only on the information relevant to human health issues as mentioned in the report.\\n    2. **Clarity of Report**: If the report is unclear or lacks detail, state that certain aspects are \\'Unable to be correctly determined based on the provided report.\\'\\n    3. **Medical Disclaimer**: Always accompany your analysis with the disclaimer: \"Consult with a Doctor before making any decisions.\"\\n\\n    Please organize your final response into the following sections:\\n    - **Identification of Disease Symptoms**\\n    - **Diagnosis and Description**\\n    - **Treatment Recommendations**\\n    - **Guidance on Medical Consultation**\\n\\n    Ensure your analysis is precise, clear, and adheres to the guidelines provided.\\n    \"\"\"\\n]\\n\\n\\ngeneration_config = {\\n  \"temperature\": 1,\\n  \"top_p\": 0.95,\\n  \"top_k\": 0,\\n  \"max_output_tokens\": 8192,\\n}\\n\\nsafety_settings = [\\n  {\\n    \"category\": \"HARM_CATEGORY_HARASSMENT\",\\n    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\\n  },\\n  {\\n    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\\n    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\\n  },\\n  {\\n    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\\n    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\\n  },\\n  {\\n    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\\n    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\\n  },\\n]', 'src\\\\__init__.py': ''}\n"
     ]
    }
   ],
   "source": [
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
